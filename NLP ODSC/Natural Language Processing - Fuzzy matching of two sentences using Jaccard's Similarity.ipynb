{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NATURAL LANGUAGE PROCESSING - FUZZY MATCHING OF TWO SENTENCES\n",
    "\n",
    "# In this micro project we will see the similarity of two sentences, both intending the same meaning but are expressed in\n",
    "# different voices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assigning a variable 'a' to first sentence\n",
    "a = \"If I don't buy some new music every month, I get bored with my collection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assigning a variable 'b' to second sentence\n",
    "b = \"I get bored with my collection so I buy some new music every month.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing NLTK library and its module to perform basic text operations\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer  #tokenizer\n",
    "from nltk.stem import SnowballStemmer  #stemmer\n",
    "from nltk.corpus import stopwords #stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) #setting and selecting stopwords to be in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer() #assigning WordPunctTokenizer function to be a variable. \n",
    "\n",
    "#This is required because we can't specify the texts inside WordPunctTokenizer() - like this --> WordPunctTokenizer(text).\n",
    "#Doing this would give error. Check the below code set to understand the error - \n",
    "\n",
    "\n",
    "# import nltk\n",
    "# text = \"Hi there! I'm going out for shopping. Would you like to come?\"\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# word_punct_tokenize = WordPunctTokenizer(text)\n",
    "\n",
    "# TypeErrorTraceback (most recent call last)\n",
    "# <ipython-input-4-440639a1a2e5> in <module>()\n",
    "# ----> 1 word_punct_tokenize = WordPunctTokenizer(text)\n",
    "\n",
    "# TypeError: __init__() takes exactly 1 argument (2 given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english') #Specifying that we want to use Snowball Stemmer and we want to stem in english. Also, assigning it to a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'buy',\n",
       " 'new',\n",
       " 'music',\n",
       " 'every',\n",
       " 'month',\n",
       " ',',\n",
       " 'get',\n",
       " 'bored',\n",
       " 'collection',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing the sentence assigned to the variable 'a' based on the stopwords present in English within NLTK\n",
    "tokens_a = [token.lower() for token in tokenizer.tokenize(a) \\\n",
    "                    if token.lower() not in stop_words]  \n",
    "\n",
    "# fetching the output\n",
    "tokens_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get', 'bored', 'collection', 'buy', 'new', 'music', 'every', 'month', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing the sentence assigned to the variable 'b' based on the stopwords present in English within NLTK\n",
    "tokens_b = [token.lower() for token in tokenizer.tokenize(b) \\\n",
    "                    if token.lower() not in stop_words]\n",
    "\n",
    "# fetching the output\n",
    "tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " u'buy',\n",
       " u'new',\n",
       " u'music',\n",
       " u'everi',\n",
       " u'month',\n",
       " ',',\n",
       " u'get',\n",
       " u'bore',\n",
       " u'collect',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming sentence 'a' based on the 'tokens_a'\n",
    "stems_a = [stemmer.stem(token) for token in tokens_a]\n",
    "\n",
    "# fetching the output\n",
    "stems_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'get',\n",
       " u'bore',\n",
       " u'collect',\n",
       " u'buy',\n",
       " u'new',\n",
       " u'music',\n",
       " u'everi',\n",
       " u'month',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming sentence 'b' based on the 'tokens_b'\n",
    "stems_b = [stemmer.stem(token) for token in tokens_b]\n",
    "\n",
    "# fetching the output\n",
    "stems_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the similarity of the two sentence using jaccard's similarity coefficient based on their stems \n",
    "jaccard_distance = len(set(stems_a).intersection(stems_b)) / float(len(set(stems_a).union(stems_b)))\n",
    "\n",
    "# fetching the output\n",
    "jaccard_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As per Jaccard's similarity coefficient, the closer the value is to 1 the more is the similarity. Hence, as per our result\n",
    "# above, we can safely conclude that the two sentences are almost similar. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
